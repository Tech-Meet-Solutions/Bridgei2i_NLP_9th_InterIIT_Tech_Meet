{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy # linear algebra\nimport pandas as pd \nimport tensorflow as tf# Get the GPU device name.\nimport torch# If there's a GPU available...\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig,BertTokenizer, BertModel, glue_convert_examples_to_features\nfrom torch.utils.data import TensorDataset, random_split,DataLoader, RandomSampler, SequentialSampler\nimport tensorflow_datasets as tfds\nfrom sklearn import metrics\nimport itertools\n# SELECTING THE TOKENIZER, THE MODEL AND THE NUMBER OF EPOCHS   #######################################################################\nEpochs=50\n\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-multilingual-cased\", \n    num_labels = 3,\n    output_attentions = False,\n    output_hidden_states = False, \n)\n\n\n\n# DEFINING FUNCTIONS #######################################################################\ndef to_sentiment(rating):\n    rating = int(rating)\n    if rating == -1:\n        return 0\n    elif rating == 0:\n        return 1\n    elif rating == 1:\n        return 2\n    elif rating == 2:\n        return 3\ndef flat_accuracy(preds, labels):\n    p=[]\n    for i in preds:\n        i=i.cpu().detach().numpy()\n        p.append(i.argmax())\n    labels_flat = labels.flatten().cpu().numpy()\n    return numpy.sum(p == labels_flat) / len(labels_flat)\ndef flat_accuracy_v2(preds, labels):\n    p=[]\n    for i in preds:\n        i=i.cpu().detach().numpy()\n        p.append(i.argmax())\n    labels_flat = labels.flatten().cpu().numpy()\n    \n    return numpy.sum(p == labels_flat) / len(labels_flat),labels_flat,p\n\n# SELECTING DATASETS FOR TRAINING AND TESTING #######################################################################\n    \ndf_EN = pd.read_csv(\"../input/mbert-5-lang-absa/ABSA_MBERT_21 - English.csv\")\ndf_HI = pd.read_csv(\"../input/mbert-5-lang-absa/Semeval16 Eng to HindiBangla and Tamil - Hindi.csv\")\ndf = pd.concat([df_EN,df_HI],ignore_index=True)\ndf = df.sample(frac = 1) \n\n\ndftrain= pd.read_csv(\"../input/iiittweets/iIIT_tweets_train.csv\")\ndftest = pd.read_csv(\"../input/iiittweets/iIIT_tweets_test.csv\")\ndftest = pd.concat([dftest,dftrain],ignore_index=True)\n# dftest = pd.read_csv(\"../input/iiittweets/I-IIT NLP annotations - Concatenated Cleaned.csv\")\ndftest['sentiment'] = dftest.sentiment.apply(to_sentiment)\n# ====================================================================================\n\n\nif torch.cuda.is_available():        # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")    \n    print('There are %d GPU(s) available.' % torch.cuda.device_count())    \n    print('We will use the GPU:', torch.cuda.get_device_name(0))# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")\n    \nmodel.to(device)\n    \nl=[]\nc=0\nfor i in df.reviews:\n    for k in i:\n        if k==\" \":\n            c+=1\n    l.append(c)\n#     print(c,i)\n    c=0\nmlen=max(l)+5\n\n\n\ndf['sentiment'] = df.sentiment.apply(to_sentiment)\nclass_names = ['negative', 'neutral', 'positive']\n\n# ENCODING THE TRAIN DATA #######################################################################\ninput_ids = []\nattention_masks = []\n\n\nfor sent,asp in zip(df['reviews'],df['aspect']):\n    encoded_dict = tokenizer.encode_plus(\n                        sent,\n                        asp,\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = min(mlen,270),           # Pad & truncate all sentences.\n                        pad_to_max_length = True,\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                   )\n       \n    input_ids.append(encoded_dict['input_ids'])\n    \n    attention_masks.append(encoded_dict['attention_mask'])\n\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\nlabels = torch.tensor(df['sentiment'])\n\n# CREATING TRAIN DATALOADERS #######################################################################\n\ndataset = TensorDataset(input_ids, attention_masks, labels)\n\ntrain_size = int(0.65 * len(dataset))\nval_size = len(dataset) - train_size\n\n# Divide the dataset by randomly selecting samples.\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} validation samples'.format(val_size))\n\n\n\nbatch_size = 16\ntrain_dataloader = DataLoader(\n            train_dataset,  # The training samples.\n            sampler = RandomSampler(train_dataset), # Select batches randomly\n            batch_size = batch_size # Trains with this batch size.\n        )\n\nvalidation_dataloader = DataLoader(\n            val_dataset, # The validation samples.\n            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n            batch_size = batch_size # Evaluate with this batch size.\n        )\n\n# ENCODING TEST DATA AND CREATING TEST DATALOADERS #######################################################################\n\n#  #######################################################################\n\nTD_input_ids = []\nTD_attention_masks = []\n\nl=[]\nc=0\nfor i in dftest.tweet:\n    for k in i:\n        if k==\" \":\n            c+=1\n    l.append(c)\n#     print(c,i)\n    c=0\nmlen=max(l)+5\n\n\nfor sent,asp in zip(dftest['tweet'],dftest['brand']):\n    encoded_dict = tokenizer.encode_plus(\n                        sent,\n                        asp,\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = min(mlen,270),           # Pad & truncate all sentences.\n                        pad_to_max_length = True,\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                   )\n       \n    TD_input_ids.append(encoded_dict['input_ids'])\n    \n    TD_attention_masks.append(encoded_dict['attention_mask'])\n\nTD_input_ids = torch.cat(TD_input_ids, dim=0)\nTD_attention_masks = torch.cat(TD_attention_masks, dim=0)\nTD_labels = torch.tensor(dftest['sentiment'])\nTD_dataset = TensorDataset(TD_input_ids,TD_attention_masks, TD_labels)\n\n# TD_dataloader = DataLoader(\n#             TD_dataset,  \n#             sampler = RandomSampler(TD_dataset), # Select batches randomly\n#             batch_size = batch_size # Trains with this batch size.\n#         )\n\n# TRAINING PHASE #######################################################################\n\n\n# dltest=TD_dataloader\n\nTD_train_size = int(0.70 * len(TD_dataset))\nTD_val_size = len(TD_dataset) - TD_train_size\n\n# Divide the dataset by randomly selecting samples.\nTD_train_dataset, TD_val_dataset = random_split(TD_dataset, [TD_train_size, TD_val_size])\n\nprint('{:>5,} training samples'.format(TD_train_size))\nprint('{:>5,} validation samples'.format(TD_val_size))\n\n\n\nbatch_size = 16\nTD_train_dataloader = DataLoader(\n            TD_train_dataset,  # The training samples.\n            sampler = RandomSampler(TD_train_dataset), # Select batches randomly\n            batch_size = batch_size # Trains with this batch size.\n        )\n\nTD_validation_dataloader = DataLoader(\n            TD_val_dataset, # The validation samples.\n            sampler = SequentialSampler(TD_val_dataset), # Pull out batches sequentially.\n            batch_size = batch_size # Evaluate with this batch size.\n        )\n\n# --------------------------------------------------------------------------------------\n\n\n\n\nacc=[]\noptim = AdamW(model.parameters(), lr=5e-6)\nmodel.eval()\ntest_res=[]\nfor batch in validation_dataloader:\n    input_ids = batch[0].to(device)\n    attention_mask = batch[1].to(device)\n    labels = batch[2].to(device)\n    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n    test_res.append(flat_accuracy(outputs[1],labels))\nprint(\"UNTUNED ACCURACY==>\",sum(test_res)/len(test_res))\nprint(\"UNTUNED ACCURACY==>\",sum(test_res)/len(test_res))\n\n\nscore_f1=[]\nacc_s=[]\nin_sent=[]\ny_gold=[]\ny_predicted=[]\nscore_all=[]\n\nfor epoch in range(Epochs):\n    print(\"Epoch:\",epoch+1,\" of \",Epochs)\n    c=0\n    l=len(TD_train_dataloader)+len(train_dataloader)\n    model.train()\n#     for layer in list(model.parameters())[:-1]:\n#         layer.requires_grad = False\n#         print(layer)\n    train_res=[]\n    \n    model.train()\n    for batch in train_dataloader:\n        c+=1\n        \n#         print(\"Epoch:\",epoch+1,\"Running \",c,\" of \",l)\n        print(\"Progress {:2.1%}\".format(c/ l), end=\"\\r\")\n        optim.zero_grad()\n        input_ids = batch[0].to(device)\n        attention_mask = batch[1].to(device)\n        labels = batch[2].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs[0]\n        train_res.append(flat_accuracy(outputs[1],labels))\n        loss.backward()\n        optim.step()\n    model.train()\n    for batch in TD_train_dataloader:\n        c+=1\n        \n#         print(\"Epoch:\",epoch+1,\"Running \",c,\" of \",l)\n        print(\"Progress {:2.1%}\".format(c/ l), end=\"\\r\")\n        optim.zero_grad()\n        input_ids = batch[0].to(device)\n        attention_mask = batch[1].to(device)\n        labels = batch[2].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs[0]\n        train_res.append(flat_accuracy(outputs[1],labels))\n        loss.backward()\n        optim.step()\n    print(\"TRAIN ACCURACY==>\",sum(train_res)/len(train_res))\n    model.eval()\n#     test_res=[]\n#     for batch in validation_dataloader:\n#         input_ids = batch[0].to(device)\n#         attention_mask = batch[1].to(device)\n#         labels = batch[2].to(device)\n#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n#         test_res.append(flat_accuracy(outputs[1],labels))\n#     print(\"VAL ACCURACY==>\",sum(test_res)/len(test_res))\n#     acc.append(sum(test_res)/len(test_res))  \n    model.eval()\n    test_res=[]\n    label_=[]\n    predict_=[]\n    ll,pp=[],[]\n\n    in_sen=[]\n    y_gl=[]\n    y_pr=[]\n    for batch in TD_validation_dataloader:\n        \n        \n        \n        \n        label_.append(batch[2])\n        input_ids = batch[0].to(device)\n        attention_mask = batch[1].to(device)\n        labels = batch[2].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        predict_.append(flat_accuracy_v2(outputs[1],labels)[2])\n        test_res.append(flat_accuracy_v2(outputs[1],labels)[0])\n        \n        inp=[]\n        v=batch[0].cpu().numpy()\n        for i in v:\n            input_sen=tokenizer.convert_ids_to_tokens(i, skip_special_tokens= 'True')\n            input_sen=tokenizer.convert_tokens_to_string(input_sen)\n            inp.append(input_sen)\n        in_sen.append(inp)\n    \n    in_sent.append(in_sen)\n    \n    \n#     print(test_lang,\" ACCURACY==>\",sum(test_res)/len(test_res))\n    acc_s.append(sum(test_res)/len(test_res))\n\n    for i in label_:\n        for m in i:\n            ll.append(m)\n    ll=numpy.array(ll)\n\n    for i in predict_:\n        for m in i:\n            pp.append(m)\n    pp=numpy.array(pp) \n    \n    y_gold.append(ll)\n    y_predicted.append(pp)\n    \n    x=metrics.classification_report(ll,pp, digits=4, zero_division=0)\n    print(\" TEST ACCURACY:\\n \",x)\n    score_all.append(x)\n    macrof1=metrics.f1_score(ll,pp, average='macro', sample_weight=None, zero_division=0)\n#     print(\"Macro avg F1 \",macrof1)\n    score_f1.append(macrof1)\n     \n# print(acc_s.index(max(acc_s))+1)    \n# print(len(in_sent),len(y_gold),len(y_/predicted),len(score_f1))\n\n#  #######################################################################\n\nidx=score_f1.index(max(score_f1))\ndf=pd.DataFrame()\ndf[\"Input\"]=list(itertools.chain(*in_sent[idx]))\ndf[\"y_gold\"]=y_gold[idx]\ndf[\"y_predicted\"]=y_predicted[idx]\n\n# name='Train'+train_lang+'Test'+test_lang+'.csv'\n# df.to_csv(name)\n\nprint(score_all[idx])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sent=\"Still having issues (Screen Flickering) Very #Bad Exp #Pune #Xiaomi @XiaomiIndia\"\nrep=\"XiaomiIndia\"\n\n\nencoded_dict = tokenizer.encode_plus(\n                        sent,rep,\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = 40,           # Pad & truncate all sentences.\n                        pad_to_max_length = True,\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                   )\n       \ninput_id=(encoded_dict['input_ids'])\n\nattention_mask=(encoded_dict['attention_mask'])\n\ninput_id1 = input_id.to(device)\nattention_mask1 = attention_mask.to(device)\noutputs = model(input_id1, attention_mask=attention_mask1)\n# for i in outputs:\n#     print(i)\n# k=(outputs.argmax())\nprint(outputs[0].argmax())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a,b,cx,t=[],[],[],[]\nimport pandas as pd\ndf = pd.read_json (r'../input/iiit-json/evaluation_data (1).json')\n# print (df)\n# df.head()\n\nc=0\nfor i,j,k in zip(df.doc_id,df.raw_text,df.company_extractions):\n#     print(type(i),type(j),type(k))\n    if(i[0]=='a'and len(k)!=0):\n        print(len(k))\n        for item in k:\n            print(i,item,k[item],\"\\nLOOK->\",k[item][0][0],k[item][0][1][0],\"\\n\",type(k[item]))\n            \n            sent=k[item][0][0]\n            rep=k[item][0][1][0]\n            t.append(i)\n            a.append(j)\n            b.append(item)\n            encoded_dict = tokenizer.encode_plus(\n                                    rep,\n                                    add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                                    max_length = 370,           # Pad & truncate all sentences.\n                                    pad_to_max_length = True,\n                                    return_attention_mask = True,   # Construct attn. masks.\n                                    return_tensors = 'pt',     # Return pytorch tensors.\n                               )\n\n            input_id=(encoded_dict['input_ids'])\n\n            attention_mask=(encoded_dict['attention_mask'])\n\n            input_id1 = input_id.to(device)\n            attention_mask1 = attention_mask.to(device)\n            outputs = model(input_id1, attention_mask=attention_mask1)\n            # for i in outputs:\n            #     print(i)\n            # k=(outputs.argmax())\n#             print(outputs[0].argmax())\n            \n            cx.append(str(int(outputs[0].argmax())))\n#             print(i,sent,rep,\"\\n\",item,\"\\n\",int(outputs[0].argmax()))\n# print(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(a),len(b),len(cx),len(t))\noutput=pd.DataFrame()\noutput[\"Text_ID\"]=t\noutput[\"Text\"]=a\noutput[\"Brand\"]=b\noutput[\"Sentiment\"]=cx\noutput.to_csv('output2_no_contex.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a,b,cx,t=[],[],[],[]\nimport pandas as pd\ndf = pd.read_json (r'../input/iiit-json/evaluation_data (1).json')\n# print (df)\n# df.head()\nal=[]\nbl=[]\nxl=[]\nc=0\nfor i,j,k in zip(df.doc_id,df.raw_text,df.company_extractions):\n#     print(type(i),type(j),type(k))\n    if(i[0]=='a'and len(k)!=0):\n#         print(len(k))\n        for item in k:\n            ll=[]\n#             print(k[item])\n            for x in k[item]:\n                cdcd=\"\"\n                for v in x[1]:\n#                 print(i,item,x[0],cdcd)\n#                     print(item)\n#                     print(i,v)\n                    sent=v\n    #                 rep=k[item][0][1][0]\n                    t.append(i)\n                    a.append(j)\n                    b.append(item)\n                    encoded_dict = tokenizer.encode_plus(\n                                            sent,\n                                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                                            max_length = 370,           # Pad & truncate all sentences.\n                                            pad_to_max_length = True,\n                                            return_attention_mask = True,   # Construct attn. masks.\n                                            return_tensors = 'pt',     # Return pytorch tensors.\n                                       )\n\n                    input_id=(encoded_dict['input_ids'])\n\n                    attention_mask=(encoded_dict['attention_mask'])\n\n                    input_id1 = input_id.to(device)\n                    attention_mask1 = attention_mask.to(device)\n                    outputs = model(input_id1, attention_mask=attention_mask1)\n                    # for i in outputs:\n                    #     print(i)\n                    # k=(outputs.argmax())\n        #             print(outputs[0].argmax())\n\n                    cx.append(str(int(outputs[0].argmax())))\n#                     print(int(outputs[0].argmax()))\n                    ll.append(int(outputs[0].argmax()))\n            print(i,item,ll)\n            al.append(i)\n            bl.append(item)\n            xl.append(ll)\n            print(\"next item\")\n    # print(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output=pd.DataFrame()\noutput[\"Text_ID\"]=al\noutput[\"Brand\"]=bl\noutput[\"Sentiment\"]=xl\noutput.to_csv('article_output2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a,b,cx,t=[],[],[],[]\nimport pandas as pd\ndf = pd.read_json (r'../input/iiit-json/evaluation_data (1).json')\n# print (df)\n# df.head()\nal=[]\nbl=[]\nxl=[]\nc=0\nfor i,j,k in zip(df.doc_id,df.raw_text,df.company_extractions):\n#     print(type(i),type(j),type(k))\n    if(i[0]=='a'and len(k)!=0):\n#         print(len(k))\n        for item in k:\n            ll=[]\n#             print(k[item])\n            for x in k[item]:\n                cdcd=\"\"\n                for v in x[1]:\n#                 print(i,item,x[0],cdcd)\n#                     print(item)\n#                     print(i,v)\n                    sent=v\n    #                 rep=k[item][0][1][0]\n                    t.append(i)\n                    a.append(j)\n                    b.append(item)\n                    encoded_dict = tokenizer.encode_plus(\n                                            sent,\n                                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                                            max_length = 370,           # Pad & truncate all sentences.\n                                            pad_to_max_length = True,\n                                            return_attention_mask = True,   # Construct attn. masks.\n                                            return_tensors = 'pt',     # Return pytorch tensors.\n                                       )\n\n                    input_id=(encoded_dict['input_ids'])\n\n                    attention_mask=(encoded_dict['attention_mask'])\n\n                    input_id1 = input_id.to(device)\n                    attention_mask1 = attention_mask.to(device)\n                    outputs = model(input_id1, attention_mask=attention_mask1)\n                    # for i in outputs:\n                    #     print(i)\n                    # k=(outputs.argmax())\n        #             print(outputs[0].argmax())\n\n                    cx.append(str(int(outputs[0].argmax())))\n#                     print(int(outputs[0].argmax()))\n                    ll.append(int(outputs[0].argmax()))\n            print(i,item,ll)\n            al.append(i)\n            bl.append(item)\n            xl.append(ll)\n            print(\"next item\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_mobt= pd.read_csv(\"../input/iiit-predictions/eval_preds_mob_tech.csv\")\ndata_mobt.head()\n\nxcd=[]\ndse=[]\nfor i,j in zip(data_mobt.Text_ID,data_mobt.Flag):\n#     print(i,j)\n    if (j==1):\n        xcd.append(i)\nprint(len(xcd))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output=pd.DataFrame()\noutput[\"Text_ID\"]=al\noutput[\"Brand\"]=bl\noutput[\"Sentiment\"]=xl\noutput.to_csv('tweet_raw_output2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vv,vvv,vvvv=[],[],[]\nprf=[]\nfor ii,jj,kk in zip(output.Text_ID,output.Brand,output.Sentiment):\n    if ii in xcd:\n        zz=0\n        tt=2\n        \n        if zz in kk:\n            if(kk.count(zz)+1>=kk.count(tt)):\n                px=\"Negative\"\n            elif(kk.count(tt)>0):\n                px=\"Positive\"\n                \n        elif tt in kk:\n            \n#             print(kk.count(tt))\n            px=\"Positive\"\n        \n        else:\n            px=\"Neutral\"\n#             print(\"Neutral\")\n        print(ii,jj,px)\n        vv.append(ii)\n        vvv.append(jj)\n        vvvv.append(px)\n        prf.append(1)\ndede=pd.DataFrame()\ndede[\"Text_ID\"]=vv\ndede[\"Mobile_Tech_Flag_Predicted\"]=prf\ndede[\"Brands_Entity_Identified\"]=vvv\ndede[\"Sentiment_Identified\"]=vvvv\ndede.to_csv('output2_articles.csv')\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}